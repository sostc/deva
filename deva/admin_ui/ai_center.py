#!/usr/bin/env python
# coding: utf-8
"""
Deva AI åŠŸèƒ½ä¸­å¿ƒ

æä¾›ç»Ÿä¸€çš„ AI åŠŸèƒ½ä½“éªŒç•Œé¢ï¼ŒåŒ…æ‹¬ï¼š
- AI ä»£ç ç”Ÿæˆï¼ˆç­–ç•¥ã€æ•°æ®æºã€ä»»åŠ¡ï¼‰
- AI æ™ºèƒ½å¯¹è¯
- AI æ¨¡å‹é…ç½®
- AI åŠŸèƒ½æ¼”ç¤º
"""

import json
import time
from pathlib import Path

from pywebio import start_server
from pywebio.output import *
from pywebio.input import *
from pywebio.pin import *
from pywebio.session import info as session_info
from pywebio.exceptions import SessionClosedException


# ============================================================================
# AI æ¨¡å‹é…ç½®ç®¡ç†
# ============================================================================

def show_llm_config_panel(ctx):
    """æ˜¾ç¤º LLM é…ç½®é¢æ¿"""
    put_markdown("### ğŸ¤– AI æ¨¡å‹é…ç½®")
    
    NB = ctx['NB']
    toast = ctx['toast']
    put_button = ctx['put_button']
    run_async = ctx['run_async']
    
    # è·å–å½“å‰é…ç½®
    llm_config = NB('llm_config', key_mode='explicit')
    
    # æ˜¾ç¤ºå½“å‰é…ç½®çŠ¶æ€
    put_markdown("**å½“å‰é…ç½®ï¼š**")
    
    config_table = [['æ¨¡å‹', 'çŠ¶æ€', 'æ“ä½œ']]
    
    models = ['kimi', 'deepseek', 'qwen', 'gpt']
    for model in models:
        config = llm_config.get(model, {})
        status = 'âœ… å·²é…ç½®' if config.get('api_key') else 'âŒ æœªé…ç½®'
        config_table.append([
            model.upper(),
            status,
            put_button('é…ç½®', onclick=lambda m=model: run_async(show_model_config_dialog(ctx, m)), link_style=True)
        ])
    
    put_table(config_table)
    
    # å¿«æ·æ“ä½œ
    put_markdown("**å¿«æ·æ“ä½œï¼š**")
    put_row([
        put_button('ğŸ“ é…ç½® Kimi', onclick=lambda: run_async(show_model_config_dialog(ctx, 'kimi')), color='primary'),
        put_button('ğŸ“ é…ç½® DeepSeek', onclick=lambda: run_async(show_model_config_dialog(ctx, 'deepseek')), color='primary'),
        put_button('ğŸ“ é…ç½® Qwen', onclick=lambda: run_async(show_model_config_dialog(ctx, 'qwen')), color='primary'),
        put_button('ğŸ“ é…ç½® GPT', onclick=lambda: run_async(show_model_config_dialog(ctx, 'gpt')), color='primary'),
    ])
    
    # æµ‹è¯•è¿æ¥
    put_markdown("**æµ‹è¯•è¿æ¥ï¼š**")
    put_row([
        put_button('ğŸ§ª æµ‹è¯• Kimi', onclick=lambda: run_async(test_llm_connection(ctx, 'kimi')), color='success'),
        put_button('ğŸ§ª æµ‹è¯• DeepSeek', onclick=lambda: run_async(test_llm_connection(ctx, 'deepseek')), color='success'),
    ])


async def show_model_config_dialog(ctx, model_type):
    """æ˜¾ç¤ºæ¨¡å‹é…ç½®å¯¹è¯æ¡†"""
    put_markdown = ctx['put_markdown']
    input = ctx['input']
    NB = ctx['NB']
    toast = ctx['toast']
    
    llm_config = NB('llm_config', key_mode='explicit')
    current_config = llm_config.get(model_type, {})
    
    with ctx['popup'](f'é…ç½® {model_type.upper()} æ¨¡å‹', closable=True):
        put_markdown(f"### {model_type.upper()} æ¨¡å‹é…ç½®")
        
        # é…ç½®è¡¨å•
        config_data = await ctx['input_group']('æ¨¡å‹é…ç½®', [
            ctx['input']('API Key', name='api_key', type='password', 
                        value=current_config.get('api_key', ''), 
                        required=True, placeholder='è¯·è¾“å…¥ API Key'),
            ctx['input']('Base URL', name='base_url', type='text',
                        value=current_config.get('base_url', ''),
                        placeholder='https://api.moonshot.cn/v1'),
            ctx['input']('æ¨¡å‹åç§°', name='model', type='text',
                        value=current_config.get('model', 'moonshot-v1-8k'),
                        placeholder='moonshot-v1-8k'),
            ctx['actions']('æ“ä½œ', [
                {'label': 'ğŸ’¾ ä¿å­˜', 'value': 'save'},
                {'label': 'âŒ å–æ¶ˆ', 'value': 'cancel'}
            ], name='action')
        ])
        
        if config_data['action'] == 'save':
            # ä¿å­˜é…ç½®
            llm_config.upsert(model_type, {
                'api_key': config_data['api_key'],
                'base_url': config_data['base_url'],
                'model': config_data['model']
            })
            toast(f'{model_type.upper()} é…ç½®å·²ä¿å­˜', color='success')


async def test_llm_connection(ctx, model_type):
    """æµ‹è¯• LLM è¿æ¥"""
    toast = ctx['toast']
    NB = ctx['NB']
    log = ctx['log']
    
    toast(f'æ­£åœ¨æµ‹è¯• {model_type.upper()} è¿æ¥...', color='info')
    
    llm_config = NB('llm_config', key_mode='explicit')
    config = llm_config.get(model_type, {})
    
    if not config.get('api_key'):
        toast(f'{model_type.upper()} æœªé…ç½®', color='warning')
        return
    
    try:
        # ç®€å•æµ‹è¯•
        import requests
        url = config.get('base_url', '').rstrip('/') + '/chat/completions'
        headers = {
            'Authorization': f"Bearer {config.get('api_key')}",
            'Content-Type': 'application/json'
        }
        payload = {
            'model': config.get('model'),
            'messages': [{'role': 'user', 'content': 'Hello'}],
            'max_tokens': 10
        }
        
        response = requests.post(url, headers=headers, json=payload, timeout=10)
        
        if response.status_code == 200:
            toast(f'{model_type.upper()} è¿æ¥æˆåŠŸï¼', color='success')
            (f"âœ… {model_type.upper()} è¿æ¥æµ‹è¯•é€šè¿‡") >> log
        else:
            toast(f'{model_type.upper()} è¿æ¥å¤±è´¥ï¼š{response.status_code}', color='error')
            (f"âŒ {model_type.upper()} è¿æ¥å¤±è´¥ï¼š{response.status_code}") >> log
            
    except Exception as e:
        toast(f'{model_type.upper()} è¿æ¥å¼‚å¸¸ï¼š{e}', color='error')
        (f"âŒ {model_type.upper()} è¿æ¥å¼‚å¸¸ï¼š{e}") >> log


# ============================================================================
# AI ä»£ç ç”Ÿæˆ
# ============================================================================

async def show_ai_code_generator(ctx):
    """æ˜¾ç¤º AI ä»£ç ç”Ÿæˆå™¨"""
    put_markdown = ctx['put_markdown']
    put_button = ctx['put_button']
    run_async = ctx['run_async']
    
    put_markdown("### ğŸ’» AI ä»£ç ç”Ÿæˆ")
    put_markdown("é€‰æ‹©è¦ç”Ÿæˆçš„ä»£ç ç±»å‹ï¼š")
    
    # ä»£ç ç”Ÿæˆé€‰é¡¹
    code_types = [
        {'label': 'ğŸ“Š é‡åŒ–ç­–ç•¥', 'value': 'strategy', 'desc': 'ç”Ÿæˆäº¤æ˜“ç­–ç•¥ä»£ç '},
        {'label': 'ğŸ“ˆ æ•°æ®æº', 'value': 'datasource', 'desc': 'ç”Ÿæˆæ•°æ®é‡‡é›†ä»£ç '},
        {'label': 'âš™ï¸ ä»»åŠ¡', 'value': 'task', 'desc': 'ç”Ÿæˆå®šæ—¶ä»»åŠ¡ä»£ç '},
    ]
    
    # æ˜¾ç¤ºé€‰é¡¹å¡ç‰‡
    for code_type in code_types:
        with ctx['use_scope'](f"ai_code_{code_type['value']}"):
            put_markdown(f"**{code_type['label']}** - {code_type['desc']}")
    
    put_row([
        put_button('ğŸ“Š ç”Ÿæˆé‡åŒ–ç­–ç•¥', onclick=lambda: run_async(show_strategy_code_gen(ctx)), color='primary'),
        put_button('ğŸ“ˆ ç”Ÿæˆæ•°æ®æº', onclick=lambda: run_async(show_datasource_code_gen(ctx)), color='primary'),
        put_button('âš™ï¸ ç”Ÿæˆä»»åŠ¡', onclick=lambda: run_async(show_task_code_gen(ctx)), color='primary'),
    ])


async def show_strategy_code_gen(ctx):
    """æ˜¾ç¤ºç­–ç•¥ä»£ç ç”Ÿæˆç•Œé¢"""
    put_markdown = ctx['put_markdown']
    input = ctx['input']
    textarea = ctx['textarea']
    NB = ctx['NB']
    
    with ctx['popup']('AI ç”Ÿæˆé‡åŒ–ç­–ç•¥', closable=True):
        put_markdown("### ğŸ“Š AI ç”Ÿæˆé‡åŒ–ç­–ç•¥")
        
        # éœ€æ±‚è¾“å…¥
        requirement = await ctx['input_group']('ç­–ç•¥éœ€æ±‚', [
            ctx['input']('ç­–ç•¥åç§°', name='name', type='text', required=True, 
                        placeholder='ä¾‹å¦‚ï¼šåŒå‡çº¿ç­–ç•¥'),
            textarea('ç­–ç•¥æè¿°', name='description', required=True,
                    placeholder='è¯¦ç»†æè¿°ç­–ç•¥é€»è¾‘ï¼Œä¾‹å¦‚ï¼šå½“ 5 æ—¥å‡çº¿ä¸Šç©¿ 20 æ—¥å‡çº¿æ—¶ä¹°å…¥ï¼Œä¸‹ç©¿æ—¶å–å‡º'),
            ctx['input']('è¾“å…¥æ•°æ®', name='input_data', type='text',
                        value='è‚¡ç¥¨ K çº¿æ•°æ®', placeholder='ç­–ç•¥éœ€è¦çš„è¾“å…¥æ•°æ®'),
            ctx['input']('è¾“å‡ºæ ¼å¼', name='output_format', type='text',
                        value='äº¤æ˜“ä¿¡å·ï¼ˆbuy/sell/holdï¼‰', placeholder='ç­–ç•¥çš„è¾“å‡ºæ ¼å¼'),
            ctx['actions']('æ“ä½œ', [
                {'label': 'ğŸ¤– ç”Ÿæˆä»£ç ', 'value': 'generate'},
                {'label': 'âŒ å–æ¶ˆ', 'value': 'cancel'}
            ], name='action')
        ])
        
        if requirement['action'] == 'generate':
            # è°ƒç”¨ AI ç”Ÿæˆä»£ç 
            await generate_strategy_code(ctx, requirement)


async def generate_strategy_code(ctx, requirement):
    """ç”Ÿæˆç­–ç•¥ä»£ç """
    put_markdown = ctx['put_markdown']
    NB = ctx['NB']
    toast = ctx['toast']
    
    toast('æ­£åœ¨ç”Ÿæˆç­–ç•¥ä»£ç ...', color='info')
    
    # æ„å»ºæç¤ºè¯
    prompt = f"""
è¯·ç”Ÿæˆä¸€ä¸ª Deva é‡åŒ–ç­–ç•¥ä»£ç ã€‚

ç­–ç•¥ä¿¡æ¯ï¼š
- åç§°ï¼š{requirement['name']}
- æè¿°ï¼š{requirement['description']}
- è¾“å…¥æ•°æ®ï¼š{requirement['input_data']}
- è¾“å‡ºæ ¼å¼ï¼š{requirement['output_format']}

è¦æ±‚ï¼š
1. ä½¿ç”¨ Deva æ¡†æ¶çš„ StrategyUnit ç±»
2. å®ç° process æ–¹æ³•å¤„ç†æ•°æ®
3. è¿”å›äº¤æ˜“ä¿¡å·ï¼ˆbuy/sell/holdï¼‰
4. æ·»åŠ é€‚å½“çš„é”™è¯¯å¤„ç†
5. ä»£ç è¦æœ‰æ¸…æ™°çš„æ³¨é‡Š

è¯·åªè¿”å› Python ä»£ç ï¼Œä¸è¦å…¶ä»–è¯´æ˜ã€‚
"""
    
    # è°ƒç”¨ AI
    try:
        from .llm_service import get_gpt_response
        code = await get_gpt_response(prompt)
        
        # æ˜¾ç¤ºç”Ÿæˆçš„ä»£ç 
        with ctx['popup']('ç”Ÿæˆçš„ç­–ç•¥ä»£ç ', closable=True):
            put_markdown("### ğŸ“Š ç”Ÿæˆçš„ç­–ç•¥ä»£ç ")
            put_code(code)
            
            # æ“ä½œæŒ‰é’®
            put_row([
                ctx['put_button']('âœ… ä½¿ç”¨æ­¤ä»£ç ', onclick=lambda: save_strategy_code(ctx, requirement['name'], code), color='success'),
                ctx['put_button']('ğŸ“‹ å¤åˆ¶ä»£ç ', onclick=lambda: ctx['run_js'](f"navigator.clipboard.writeText(`{code}`)"), color='primary'),
                ctx['put_button']('ğŸ”„ é‡æ–°ç”Ÿæˆ', onclick=lambda: ctx['run_async'](show_strategy_code_gen(ctx)), color='warning'),
            ])
            
    except Exception as e:
        toast(f'ç”Ÿæˆå¤±è´¥ï¼š{e}', color='error')


def save_strategy_code(ctx, name, code):
    """ä¿å­˜ç­–ç•¥ä»£ç """
    toast = ctx['toast']
    log = ctx['log']
    
    # ä¿å­˜åˆ°ç­–ç•¥ç®¡ç†å™¨
    try:
        from .strategy.strategy_manager import get_strategy_manager
        mgr = get_strategy_manager()
        mgr.add_strategy(name=name, code=code)
        toast(f'ç­–ç•¥ "{name}" å·²ä¿å­˜', color='success')
        (f"âœ… ç­–ç•¥å·²ä¿å­˜ï¼š{name}") >> log
    except Exception as e:
        toast(f'ä¿å­˜å¤±è´¥ï¼š{e}', color='error')
        (f"âŒ ç­–ç•¥ä¿å­˜å¤±è´¥ï¼š{name}, é”™è¯¯ï¼š{e}") >> log


async def show_datasource_code_gen(ctx):
    """æ˜¾ç¤ºæ•°æ®æºä»£ç ç”Ÿæˆç•Œé¢"""
    put_markdown = ctx['put_markdown']
    input = ctx['input']
    textarea = ctx['textarea']
    
    with ctx['popup']('AI ç”Ÿæˆæ•°æ®æº', closable=True):
        put_markdown("### ğŸ“ˆ AI ç”Ÿæˆæ•°æ®æº")
        
        # éœ€æ±‚è¾“å…¥
        requirement = await ctx['input_group']('æ•°æ®æºéœ€æ±‚', [
            ctx['input']('æ•°æ®æºåç§°', name='name', type='text', required=True,
                        placeholder='ä¾‹å¦‚ï¼šè‚¡ç¥¨å®æ—¶æ•°æ®'),
            textarea('æ•°æ®æºæè¿°', name='description', required=True,
                    placeholder='è¯¦ç»†æè¿°æ•°æ®æºï¼Œä¾‹å¦‚ï¼šä» Yahoo Finance è·å–è‚¡ç¥¨å®æ—¶è¡Œæƒ…æ•°æ®'),
            ctx['input']('æ•°æ®ç±»å‹', name='data_type', type='text',
                        value='dict', placeholder='è¿”å›çš„æ•°æ®ç±»å‹'),
            ctx['input']('æ›´æ–°é¢‘ç‡', name='interval', type='text',
                        value='5', placeholder='æ•°æ®æ›´æ–°é—´éš”ï¼ˆç§’ï¼‰'),
            ctx['actions']('æ“ä½œ', [
                {'label': 'ğŸ¤– ç”Ÿæˆä»£ç ', 'value': 'generate'},
                {'label': 'âŒ å–æ¶ˆ', 'value': 'cancel'}
            ], name='action')
        ])
        
        if requirement['action'] == 'generate':
            await generate_datasource_code(ctx, requirement)


async def generate_datasource_code(ctx, requirement):
    """ç”Ÿæˆæ•°æ®æºä»£ç """
    toast = ctx['toast']
    
    toast('æ­£åœ¨ç”Ÿæˆæ•°æ®æºä»£ç ...', color='info')
    
    prompt = f"""
è¯·ç”Ÿæˆä¸€ä¸ª Deva æ•°æ®æºä»£ç ã€‚

æ•°æ®æºä¿¡æ¯ï¼š
- åç§°ï¼š{requirement['name']}
- æè¿°ï¼š{requirement['description']}
- æ•°æ®ç±»å‹ï¼š{requirement['data_type']}
- æ›´æ–°é¢‘ç‡ï¼š{requirement['interval']}ç§’

è¦æ±‚ï¼š
1. ä½¿ç”¨ Deva æ¡†æ¶çš„ DataSource ç±»
2. å®ç° fetch_data æ–¹æ³•è·å–æ•°æ®
3. æ·»åŠ é€‚å½“çš„é”™è¯¯å¤„ç†
4. ä»£ç è¦æœ‰æ¸…æ™°çš„æ³¨é‡Š

è¯·åªè¿”å› Python ä»£ç ï¼Œä¸è¦å…¶ä»–è¯´æ˜ã€‚
"""
    
    try:
        from .llm_service import get_gpt_response
        code = await get_gpt_response(prompt)
        
        with ctx['popup']('ç”Ÿæˆçš„æ•°æ®æºä»£ç ', closable=True):
            put_markdown("### ğŸ“ˆ ç”Ÿæˆçš„æ•°æ®æºä»£ç ")
            put_code(code)
            
            put_row([
                ctx['put_button']('âœ… ä½¿ç”¨æ­¤ä»£ç ', onclick=lambda: save_datasource_code(ctx, requirement['name'], code), color='success'),
                ctx['put_button']('ğŸ“‹ å¤åˆ¶ä»£ç ', onclick=lambda: ctx['run_js'](f"navigator.clipboard.writeText(`{code}`)"), color='primary'),
            ])
            
    except Exception as e:
        toast(f'ç”Ÿæˆå¤±è´¥ï¼š{e}', color='error')


def save_datasource_code(ctx, name, code):
    """ä¿å­˜æ•°æ®æºä»£ç """
    toast = ctx['toast']
    log = ctx['log']
    
    try:
        from .strategy.datasource_manager import get_datasource_manager
        mgr = get_datasource_manager()
        mgr.add_datasource(name=name, code=code)
        toast(f'æ•°æ®æº "{name}" å·²ä¿å­˜', color='success')
        (f"âœ… æ•°æ®æºå·²ä¿å­˜ï¼š{name}") >> log
    except Exception as e:
        toast(f'ä¿å­˜å¤±è´¥ï¼š{e}', color='error')
        (f"âŒ æ•°æ®æºä¿å­˜å¤±è´¥ï¼š{name}, é”™è¯¯ï¼š{e}") >> log


async def show_task_code_gen(ctx):
    """æ˜¾ç¤ºä»»åŠ¡ä»£ç ç”Ÿæˆç•Œé¢"""
    put_markdown = ctx['put_markdown']
    input = ctx['input']
    textarea = ctx['textarea']
    
    with ctx['popup']('AI ç”Ÿæˆä»»åŠ¡', closable=True):
        put_markdown("### âš™ï¸ AI ç”Ÿæˆä»»åŠ¡")
        
        requirement = await ctx['input_group']('ä»»åŠ¡éœ€æ±‚', [
            ctx['input']('ä»»åŠ¡åç§°', name='name', type='text', required=True,
                        placeholder='ä¾‹å¦‚ï¼šæ¯æ—¥æ•°æ®å¤‡ä»½'),
            textarea('ä»»åŠ¡æè¿°', name='description', required=True,
                    placeholder='è¯¦ç»†æè¿°ä»»åŠ¡åŠŸèƒ½'),
            ctx['input']('æ‰§è¡Œæ—¶é—´', name='schedule', type='text',
                        value='æ¯å¤© 00:00', placeholder='ä»»åŠ¡æ‰§è¡Œæ—¶é—´'),
            ctx['actions']('æ“ä½œ', [
                {'label': 'ğŸ¤– ç”Ÿæˆä»£ç ', 'value': 'generate'},
                {'label': 'âŒ å–æ¶ˆ', 'value': 'cancel'}
            ], name='action')
        ])
        
        if requirement['action'] == 'generate':
            await generate_task_code(ctx, requirement)


async def generate_task_code(ctx, requirement):
    """ç”Ÿæˆä»»åŠ¡ä»£ç """
    toast = ctx['toast']
    
    toast('æ­£åœ¨ç”Ÿæˆä»»åŠ¡ä»£ç ...', color='info')
    
    prompt = f"""
è¯·ç”Ÿæˆä¸€ä¸ª Deva ä»»åŠ¡ä»£ç ã€‚

ä»»åŠ¡ä¿¡æ¯ï¼š
- åç§°ï¼š{requirement['name']}
- æè¿°ï¼š{requirement['description']}
- æ‰§è¡Œæ—¶é—´ï¼š{requirement['schedule']}

è¦æ±‚ï¼š
1. ä½¿ç”¨ Deva æ¡†æ¶çš„ TaskUnit ç±»
2. å®ç° execute æ–¹æ³•æ‰§è¡Œä»»åŠ¡
3. æ·»åŠ é€‚å½“çš„é”™è¯¯å¤„ç†
4. ä»£ç è¦æœ‰æ¸…æ™°çš„æ³¨é‡Š

è¯·åªè¿”å› Python ä»£ç ï¼Œä¸è¦å…¶ä»–è¯´æ˜ã€‚
"""
    
    try:
        from .llm_service import get_gpt_response
        code = await get_gpt_response(prompt)
        
        with ctx['popup']('ç”Ÿæˆçš„ä»»åŠ¡ä»£ç ', closable=True):
            put_markdown("### âš™ï¸ ç”Ÿæˆçš„ä»»åŠ¡ä»£ç ")
            put_code(code)
            
            put_row([
                ctx['put_button']('âœ… ä½¿ç”¨æ­¤ä»£ç ', onclick=lambda: save_task_code(ctx, requirement['name'], code), color='success'),
                ctx['put_button']('ğŸ“‹ å¤åˆ¶ä»£ç ', onclick=lambda: ctx['run_js'](f"navigator.clipboard.writeText(`{code}`)"), color='primary'),
            ])
            
    except Exception as e:
        toast(f'ç”Ÿæˆå¤±è´¥ï¼š{e}', color='error')


def save_task_code(ctx, name, code):
    """ä¿å­˜ä»»åŠ¡ä»£ç """
    toast = ctx['toast']
    log = ctx['log']
    
    try:
        from .strategy.task_manager import get_task_manager
        mgr = get_task_manager()
        mgr.add_task(name=name, code=code)
        toast(f'ä»»åŠ¡ "{name}" å·²ä¿å­˜', color='success')
        (f"âœ… ä»»åŠ¡å·²ä¿å­˜ï¼š{name}") >> log
    except Exception as e:
        toast(f'ä¿å­˜å¤±è´¥ï¼š{e}', color='error')
        (f"âŒ ä»»åŠ¡ä¿å­˜å¤±è´¥ï¼š{name}, é”™è¯¯ï¼š{e}") >> log


# ============================================================================
# AI æ™ºèƒ½å¯¹è¯
# ============================================================================

async def show_ai_chat(ctx):
    """æ˜¾ç¤º AI æ™ºèƒ½å¯¹è¯ç•Œé¢"""
    put_markdown = ctx['put_markdown']
    input = ctx['input']
    run_async = ctx['run_async']
    
    put_markdown("### ğŸ¤– AI æ™ºèƒ½å¯¹è¯")
    
    # åˆå§‹åŒ–å¯¹è¯å†å²
    if 'chat_history' not in ctx:
        ctx['chat_history'] = []
    
    # æ˜¾ç¤ºå¯¹è¯å†å²
    with ctx['use_scope']('chat_history'):
        for msg in ctx['chat_history'][-10:]:  # åªæ˜¾ç¤ºæœ€è¿‘ 10 æ¡
            if msg['role'] == 'user':
                put_markdown(f"**ğŸ‘¤ ä½ ï¼š** {msg['content']}")
            else:
                put_markdown(f"**ğŸ¤– AIï¼š** {msg['content']}")
    
    # è¾“å…¥æ¡†
    user_input = await ctx['input_group']('è¾“å…¥æ¶ˆæ¯', [
        ctx['input']('æ¶ˆæ¯å†…å®¹', name='message', type='text', 
                    placeholder='è¾“å…¥ä½ æƒ³é—®çš„é—®é¢˜...', required=True),
        ctx['actions']('å‘é€', [
            {'label': 'ğŸ“¤ å‘é€', 'value': 'send'},
        ], name='action')
    ])
    
    if user_input['action'] == 'send' and user_input['message']:
        await process_chat_message(ctx, user_input['message'])


async def process_chat_message(ctx, message):
    """å¤„ç†å¯¹è¯æ¶ˆæ¯"""
    put_markdown = ctx['put_markdown']
    NB = ctx['NB']
    
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
    ctx['chat_history'].append({'role': 'user', 'content': message})
    
    # æ¸…ç©ºè¾“å…¥å†å²
    ctx['clear']('chat_history')
    
    # æ˜¾ç¤ºæ€è€ƒä¸­
    with ctx['use_scope']('thinking'):
        put_markdown("*ğŸ¤– AI æ­£åœ¨æ€è€ƒ...*")
    
    # è°ƒç”¨ AI
    try:
        from .llm_service import get_gpt_response
        
        # æ„å»ºå¯¹è¯å†å²
        messages = []
        for msg in ctx['chat_history'][-5:]:  # ä½¿ç”¨æœ€è¿‘ 5 æ¡å†å²
            messages.append({
                'role': msg['role'],
                'content': msg['content']
            })
        
        # è·å– AI å“åº”
        response = await get_gpt_response(messages=messages)
        
        # æ·»åŠ  AI å“åº”åˆ°å†å²
        ctx['chat_history'].append({'role': 'assistant', 'content': response})
        
        # æ›´æ–°æ˜¾ç¤º
        ctx['clear']('thinking')
        ctx['clear']('chat_history')
        
        with ctx['use_scope']('chat_history'):
            for msg in ctx['chat_history'][-10:]:
                if msg['role'] == 'user':
                    put_markdown(f"**ğŸ‘¤ ä½ ï¼š** {msg['content']}")
                else:
                    put_markdown(f"**ğŸ¤– AIï¼š** {msg['content']}")
        
        # é‡æ–°æ˜¾ç¤ºè¾“å…¥æ¡†
        await show_ai_chat(ctx)
        
    except Exception as e:
        ctx['clear']('thinking')
        put_markdown(f"âŒ é”™è¯¯ï¼š{e}")


# ============================================================================
# AI åŠŸèƒ½æ¼”ç¤º
# ============================================================================

async def show_ai_demos(ctx):
    """æ˜¾ç¤º AI åŠŸèƒ½æ¼”ç¤º"""
    put_markdown = ctx['put_markdown']
    put_button = ctx['put_button']
    run_async = ctx['run_async']
    
    put_markdown("### ğŸ¯ AI åŠŸèƒ½æ¼”ç¤º")
    put_markdown("ä½“éªŒ Deva çš„ AI åŠŸèƒ½ï¼š")
    
    # æ¼”ç¤ºé€‰é¡¹
    demos = [
        {'label': 'ğŸ“ æ–‡ç« æ‘˜è¦', 'value': 'summary', 'desc': 'è‡ªåŠ¨ç”Ÿæˆæ–‡ç« æ‘˜è¦'},
        {'label': 'ğŸ”— é“¾æ¥æå–', 'value': 'links', 'desc': 'æ™ºèƒ½æå–é‡è¦é“¾æ¥'},
        {'label': 'ğŸ“Š æ•°æ®åˆ†æ', 'value': 'analysis', 'desc': 'AI åˆ†ææ•°æ®è¶‹åŠ¿'},
        {'label': 'ğŸŒ æ–°é—»ç¿»è¯‘', 'value': 'translate', 'desc': 'è‡ªåŠ¨ç¿»è¯‘å¤–æ–‡æ–°é—»'},
    ]
    
    for demo in demos:
        with ctx['use_scope'](f"ai_demo_{demo['value']}"):
            put_markdown(f"**{demo['label']}** - {demo['desc']}")
    
    put_row([
        put_button('ğŸ“ æ–‡ç« æ‘˜è¦', onclick=lambda: run_async(demo_article_summary(ctx)), color='info'),
        put_button('ğŸ”— é“¾æ¥æå–', onclick=lambda: run_async(demo_link_extraction(ctx)), color='info'),
        put_button('ğŸ“Š æ•°æ®åˆ†æ', onclick=lambda: run_async(demo_data_analysis(ctx)), color='info'),
        put_button('ğŸŒ æ–°é—»ç¿»è¯‘', onclick=lambda: run_async(demo_translation(ctx)), color='info'),
    ])


async def demo_article_summary(ctx):
    """æ¼”ç¤ºæ–‡ç« æ‘˜è¦"""
    put_markdown = ctx['put_markdown']
    textarea = ctx['textarea']
    
    with ctx['popup']('æ–‡ç« æ‘˜è¦', closable=True):
        put_markdown("### ğŸ“ æ–‡ç« æ‘˜è¦ç”Ÿæˆ")
        
        article = await ctx['input_group']('è¾“å…¥æ–‡ç« ', [
            textarea('æ–‡ç« å†…å®¹', name='content', required=True,
                    placeholder='ç²˜è´´æ–‡ç« å†…å®¹...', rows=10),
            ctx['actions']('æ“ä½œ', [
                {'label': 'ğŸ¤– ç”Ÿæˆæ‘˜è¦', 'value': 'generate'},
            ], name='action')
        ])
        
        if article['action'] == 'generate':
            toast = ctx['toast']
            toast('æ­£åœ¨ç”Ÿæˆæ‘˜è¦...', color='info')
            
            try:
                from .llm_service import get_gpt_response
                prompt = f"è¯·ç”¨ä¸€å¥è¯æ€»ç»“ä»¥ä¸‹æ–‡ç« ï¼š\n\n{article['content']}"
                summary = await get_gpt_response(prompt)
                
                put_markdown("### æ‘˜è¦")
                put_markdown(f"> {summary}")
                
            except Exception as e:
                toast(f'ç”Ÿæˆå¤±è´¥ï¼š{e}', color='error')


async def demo_link_extraction(ctx):
    """æ¼”ç¤ºé“¾æ¥æå–"""
    put_markdown = ctx['put_markdown']
    textarea = ctx['textarea']
    
    with ctx['popup']('é“¾æ¥æå–', closable=True):
        put_markdown("### ğŸ”— æ™ºèƒ½é“¾æ¥æå–")
        
        html = await ctx['input_group']('è¾“å…¥ HTML', [
            textarea('HTML å†…å®¹', name='html', required=True,
                    placeholder='ç²˜è´´ HTML ä»£ç ...', rows=10),
            ctx['actions']('æ“ä½œ', [
                {'label': 'ğŸ¤– æå–é“¾æ¥', 'value': 'extract'},
            ], name='action')
        ])
        
        if html['action'] == 'extract':
            toast = ctx['toast']
            toast('æ­£åœ¨æå–é“¾æ¥...', color='info')
            
            try:
                from .llm_service import get_gpt_response
                prompt = f"ä»ä»¥ä¸‹ HTML ä¸­æå–æœ€é‡è¦çš„ 3 ä¸ªé“¾æ¥ï¼Œè¿”å› JSON æ ¼å¼ï¼š\n\n{html['html']}"
                result = await get_gpt_response(prompt)
                
                put_markdown("### æå–çš„é“¾æ¥")
                put_code(result)
                
            except Exception as e:
                toast(f'æå–å¤±è´¥ï¼š{e}', color='error')


async def demo_data_analysis(ctx):
    """æ¼”ç¤ºæ•°æ®åˆ†æ"""
    put_markdown = ctx['put_markdown']
    textarea = ctx['textarea']
    
    with ctx['popup']('æ•°æ®åˆ†æ', closable=True):
        put_markdown("### ğŸ“Š AI æ•°æ®åˆ†æ")
        
        data = await ctx['input_group']('è¾“å…¥æ•°æ®', [
            textarea('æ•°æ®', name='data', required=True,
                    placeholder='ç²˜è´´æ•°æ®ï¼ˆCSVã€JSON ç­‰ï¼‰...', rows=10),
            ctx['actions']('æ“ä½œ', [
                {'label': 'ğŸ¤– åˆ†ææ•°æ®', 'value': 'analyze'},
            ], name='action')
        ])
        
        if data['action'] == 'analyze':
            toast = ctx['toast']
            toast('æ­£åœ¨åˆ†ææ•°æ®...', color='info')
            
            try:
                from .llm_service import get_gpt_response
                prompt = f"åˆ†æä»¥ä¸‹æ•°æ®ï¼Œæ‰¾å‡ºå…³é”®è¶‹åŠ¿å’Œæ´å¯Ÿï¼š\n\n{data['data']}"
                analysis = await get_gpt_response(prompt)
                
                put_markdown("### åˆ†æç»“æœ")
                put_markdown(analysis)
                
            except Exception as e:
                toast(f'åˆ†æå¤±è´¥ï¼š{e}', color='error')


async def demo_translation(ctx):
    """æ¼”ç¤ºç¿»è¯‘"""
    put_markdown = ctx['put_markdown']
    textarea = ctx['textarea']
    
    with ctx['popup']('ç¿»è¯‘', closable=True):
        put_markdown("### ğŸŒ AI ç¿»è¯‘")
        
        text_data = await ctx['input_group']('è¾“å…¥æ–‡æœ¬', [
            textarea('åŸæ–‡', name='text', required=True,
                    placeholder='ç²˜è´´è¦ç¿»è¯‘çš„æ–‡æœ¬...', rows=10),
            ctx['input']('ç›®æ ‡è¯­è¨€', name='target', type='text',
                        value='ä¸­æ–‡', placeholder='ä¾‹å¦‚ï¼šä¸­æ–‡ã€è‹±æ–‡'),
            ctx['actions']('æ“ä½œ', [
                {'label': 'ğŸ¤– ç¿»è¯‘', 'value': 'translate'},
            ], name='action')
        ])
        
        if text_data['action'] == 'translate':
            toast = ctx['toast']
            toast('æ­£åœ¨ç¿»è¯‘...', color='info')
            
            try:
                from .llm_service import get_gpt_response
                prompt = f"å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘æˆ{text_data['target']}ï¼š\n\n{text_data['text']}"
                translation = await get_gpt_response(prompt)
                
                put_markdown("### ç¿»è¯‘ç»“æœ")
                put_markdown(translation)
                
            except Exception as e:
                toast(f'ç¿»è¯‘å¤±è´¥ï¼š{e}', color='error')


# ============================================================================
# AI Tab ä¸»ç•Œé¢
# ============================================================================

def render_ai_tab_ui(ctx):
    """æ¸²æŸ“ AI Tab ä¸»ç•Œé¢"""
    put_markdown = ctx['put_markdown']
    put_tabs = ctx['put_tabs']
    run_async = ctx['run_async']
    
    put_markdown("## ğŸ¤– AI åŠŸèƒ½ä¸­å¿ƒ")
    put_markdown("ä½“éªŒ Deva çš„å¼ºå¤§ AI åŠŸèƒ½")
    
    # æ„å»º Tabs
    tabs = [
        {
            'title': 'ğŸ¤– æ¨¡å‹é…ç½®',
            'content': show_llm_config_panel(ctx)
        },
        {
            'title': 'ğŸ’» ä»£ç ç”Ÿæˆ',
            'content': run_async(show_ai_code_generator(ctx))
        },
        {
            'title': 'ğŸ’¬ æ™ºèƒ½å¯¹è¯',
            'content': run_async(show_ai_chat(ctx))
        },
        {
            'title': 'ğŸ¯ åŠŸèƒ½æ¼”ç¤º',
            'content': run_async(show_ai_demos(ctx))
        }
    ]
    
    put_tabs(tabs)


if __name__ == '__main__':
    # æµ‹è¯•è¿è¡Œ
    from pywebio import start_server
    from pywebio.output import *
    
    def test_ai_tab():
        render_ai_tab_ui({
            'put_markdown': put_markdown,
            'put_tabs': put_tabs,
            'run_async': lambda f: f(),
            'input': input,
            'textarea': textarea,
            'input_group': input_group,
            'popup': popup,
            'toast': toast,
            'put_button': put_button,
            'put_table': put_table,
            'put_row': put_row,
            'put_code': put_code,
            'clear': clear,
            'use_scope': use_scope,
            'run_js': run_js,
            'NB': None,
            'log': None,
        })
    
    start_server(test_ai_tab, port=8080, debug=True)
