#!/usr/bin/env python
# coding: utf-8
"""
Deva AI åŠŸèƒ½ä¸­å¿ƒ

æ•´åˆ Deva ä»£ç åº“ä¸­æ‰€æœ‰ LLM å’Œ AI ç›¸å…³åŠŸèƒ½ï¼Œæä¾›å¯è§†åŒ– UI ä½“éªŒç•Œé¢ã€‚

åŠŸèƒ½æ¨¡å—ï¼š
1. AI æ¨¡å‹é…ç½® - é…ç½®å’Œç®¡ç† LLM æ¨¡å‹
2. AI æ™ºèƒ½å¯¹è¯ - ä¸ AI è¿›è¡Œå¤šè½®å¯¹è¯
3. AI ä»£ç ç”Ÿæˆ - ç”Ÿæˆ Python/Deva ä»£ç 
4. AI æ–‡æœ¬å¤„ç† - æ‘˜è¦ã€ç¿»è¯‘ã€æ¶¦è‰²ç­‰
5. AI JSON å¤„ç† - JSON æ ¼å¼æ•°æ®ç”Ÿæˆå’Œè§£æ
6. AI æµ‹è¯•å·¥å…· - æµ‹è¯• AI åŠŸèƒ½å’Œæ€§èƒ½
"""

from __future__ import annotations

import json
import time
from datetime import datetime
from typing import Dict, Any, List, Optional


# ============================================================================
# AI æ¨¡å‹é…ç½®ç®¡ç†
# ============================================================================

def show_llm_config_panel(ctx):
    """æ˜¾ç¤º LLM æ¨¡å‹é…ç½®é¢æ¿"""
    put_markdown = ctx['put_markdown']
    put_table = ctx['put_table']
    put_button = ctx['put_button']
    put_row = ctx['put_row']
    run_async = ctx['run_async']
    NB = ctx['NB']
    
    put_markdown("### ğŸ¤– AI æ¨¡å‹é…ç½®")
    put_markdown("é…ç½®å’Œç®¡ç†å¤§å‹è¯­è¨€æ¨¡å‹è¿æ¥ä¿¡æ¯")
    
    # è·å–é…ç½®çŠ¶æ€
    from deva.llm.config_utils import get_model_config_status, build_model_config_example
    
    config = NB('llm_config', key_mode='explicit')
    
    # æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨
    models = [
        {'name': 'DeepSeek', 'type': 'deepseek', 'default_url': 'https://api.deepseek.com/v1', 'default_model': 'deepseek-chat'},
        {'name': 'Kimi (æœˆä¹‹æš—é¢)', 'type': 'kimi', 'default_url': 'https://api.moonshot.cn/v1', 'default_model': 'moonshot-v1-8k'},
        {'name': 'Sambanova', 'type': 'sambanova', 'default_url': 'https://api.sambanova.ai/v1', 'default_model': 'Meta-Llama-3.1-70B-Instruct'},
        {'name': 'Qwen (é€šä¹‰åƒé—®)', 'type': 'qwen', 'default_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1', 'default_model': 'qwen-plus'},
    ]
    
    # æ˜¾ç¤ºé…ç½®çŠ¶æ€è¡¨æ ¼
    table_data = [["æ¨¡å‹", "çŠ¶æ€", "API Key", "Base URL", "æ¨¡å‹åç§°", "æ“ä½œ"]]
    
    for model in models:
        model_config = config.get(model['type'], {})
        api_key = model_config.get('api_key', '')
        base_url = model_config.get('base_url', '')
        model_name = model_config.get('model', '')
        
        # çŠ¶æ€åˆ¤æ–­
        if api_key and base_url and model_name:
            status = '<span style="color:#28a745">âœ… å·²é…ç½®</span>'
        elif api_key:
            status = '<span style="color:#ffc107">âš ï¸ éƒ¨åˆ†é…ç½®</span>'
        else:
            status = '<span style="color:#dc3545">âŒ æœªé…ç½®</span>'
        
        # è„±æ•æ˜¾ç¤º API Key
        api_key_display = f"{api_key[:4]}...{api_key[-4:]}" if api_key else '-'
        
        table_data.append([
            model['name'],
            status,
            api_key_display,
            base_url[:40] + '...' if len(base_url) > 40 else base_url,
            model_name,
            put_button('é…ç½®', onclick=lambda m=model['type']: run_async(show_model_config_dialog(ctx, m)), link_style=True)
        ])
    
    put_table(table_data)
    
    # å¿«æ·æ“ä½œ
    put_markdown("**å¿«æ·æ“ä½œï¼š**")
    put_row([
        put_button('ğŸ“ é…ç½® DeepSeek', onclick=lambda: run_async(show_model_config_dialog(ctx, 'deepseek')), color='primary'),
        put_button('ğŸ“ é…ç½® Kimi', onclick=lambda: run_async(show_model_config_dialog(ctx, 'kimi')), color='primary'),
        put_button('ğŸ§ª æµ‹è¯•è¿æ¥', onclick=lambda: run_async(test_llm_connection(ctx)), color='success'),
        put_button('ğŸ“– é…ç½®æŒ‡å—', onclick=lambda: show_config_guide(ctx), color='info'),
    ])


async def show_model_config_dialog(ctx, model_type):
    """æ˜¾ç¤ºæ¨¡å‹é…ç½®å¯¹è¯æ¡†"""
    put_markdown = ctx['put_markdown']
    input = ctx['input']
    NB = ctx['NB']
    toast = ctx['toast']
    
    # è·å–å½“å‰é…ç½®
    config = NB('llm_config', key_mode='explicit')
    current_config = config.get(model_type, {})
    
    # é»˜è®¤é…ç½®
    defaults = {
        'deepseek': {'base_url': 'https://api.deepseek.com/v1', 'model': 'deepseek-chat'},
        'kimi': {'base_url': 'https://api.moonshot.cn/v1', 'model': 'moonshot-v1-8k'},
        'sambanova': {'base_url': 'https://api.sambanova.ai/v1', 'model': 'Meta-Llama-3.1-70B-Instruct'},
        'qwen': {'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1', 'model': 'qwen-plus'},
    }
    
    default = defaults.get(model_type, {})
    
    with ctx['popup'](f'é…ç½® {model_type.upper()} æ¨¡å‹', size='large', closable=True):
        put_markdown(f"### {model_type.upper()} æ¨¡å‹é…ç½®")
        
        put_markdown("""
        ğŸ’¡ **æç¤ºï¼š**
        - API Key å¯ä»¥ä»å¯¹åº”å¹³å°çš„æ§åˆ¶å°è·å–
        - Base URL æ˜¯ API æœåŠ¡çš„åœ°å€
        - æ¨¡å‹åç§°æ˜¯å…·ä½“ä½¿ç”¨çš„æ¨¡å‹
        """)
        
        # é…ç½®è¡¨å•
        config_data = await ctx['input_group']('æ¨¡å‹é…ç½®', [
            input('API Key', name='api_key', type='password', 
                  value=current_config.get('api_key', ''), 
                  required=True, placeholder='è¯·è¾“å…¥ API Key'),
            input('Base URL', name='base_url', type='text',
                  value=current_config.get('base_url', default['base_url']),
                  placeholder='https://api.example.com/v1'),
            input('æ¨¡å‹åç§°', name='model', type='text',
                  value=current_config.get('model', default['model']),
                  placeholder='model-name'),
            ctx['actions']('æ“ä½œ', [
                {'label': 'ğŸ’¾ ä¿å­˜', 'value': 'save'},
                {'label': 'âŒ å–æ¶ˆ', 'value': 'cancel'}
            ], name='action')
        ])
        
        if config_data['action'] == 'save':
            # ä¿å­˜é…ç½®
            config.upsert(model_type, {
                'api_key': config_data['api_key'],
                'base_url': config_data['base_url'],
                'model': config_data['model']
            })
            toast(f'{model_type.upper()} é…ç½®å·²ä¿å­˜', color='success')
            
            # åˆ·æ–°é…ç½®é¢æ¿
            ctx['clear']('llm_config_panel')
            with ctx['use_scope']('llm_config_panel'):
                show_llm_config_panel(ctx)


async def test_llm_connection(ctx):
    """æµ‹è¯• LLM è¿æ¥"""
    put_markdown = ctx['put_markdown']
    toast = ctx['toast']
    NB = ctx['NB']
    log = ctx['log']
    
    with ctx['popup']('æµ‹è¯• AI è¿æ¥', size='large', closable=True):
        put_markdown("### ğŸ§ª æµ‹è¯• AI è¿æ¥")
        
        # é€‰æ‹©è¦æµ‹è¯•çš„æ¨¡å‹
        model_type = await ctx['radio']('é€‰æ‹©è¦æµ‹è¯•çš„æ¨¡å‹', 
            options=[
                {'label': 'DeepSeek', 'value': 'deepseek'},
                {'label': 'Kimi', 'value': 'kimi'},
                {'label': 'Sambanova', 'value': 'sambanova'},
            ],
            value='deepseek'
        )
        
        # æµ‹è¯•é—®é¢˜
        test_prompt = await ctx['input']('æµ‹è¯•é—®é¢˜', value='ä½ å¥½ï¼Œè¯·ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±', placeholder='è¾“å…¥æµ‹è¯•é—®é¢˜')
        
        put_markdown("**å¼€å§‹æµ‹è¯•...**")
        
        try:
            # è°ƒç”¨ AI
            from deva.admin_ui.llm_service import get_gpt_response
            response = await get_gpt_response(ctx, test_prompt, model_type=model_type)
            
            if response:
                put_markdown("### âœ… è¿æ¥æˆåŠŸ")
                put_markdown(f"**AI å›å¤ï¼š** {response}")
                toast(f'{model_type.upper()} è¿æ¥æµ‹è¯•æˆåŠŸ', color='success')
            else:
                put_markdown("### âŒ è¿æ¥å¤±è´¥")
                put_markdown("AI è¿”å›ä¸ºç©º")
                toast(f'{model_type.upper()} è¿æ¥æµ‹è¯•å¤±è´¥', color='error')
                
        except Exception as e:
            put_markdown(f"### âŒ è¿æ¥å¼‚å¸¸")
            put_markdown(f"**é”™è¯¯ä¿¡æ¯ï¼š** {str(e)}")
            toast(f'{model_type.upper()} è¿æ¥å¼‚å¸¸ï¼š{e}', color='error')


def show_config_guide(ctx):
    """æ˜¾ç¤ºé…ç½®æŒ‡å—"""
    with ctx['popup']('é…ç½®æŒ‡å—', size='large', closable=True):
        ctx['put_markdown']("### ğŸ“– AI æ¨¡å‹é…ç½®æŒ‡å—")
        
        ctx['put_markdown']("""
        #### 1. DeepSeek (æ·±åº¦æ±‚ç´¢)
        
        **è·å– API Keyï¼š**
        1. è®¿é—® https://platform.deepseek.com/
        2. æ³¨å†Œ/ç™»å½•è´¦å·
        3. è¿›å…¥æ§åˆ¶å° -> API Keys
        4. åˆ›å»º API Key
        
        **é…ç½®ä¿¡æ¯ï¼š**
        - Base URL: `https://api.deepseek.com/v1`
        - æ¨¡å‹ï¼š`deepseek-chat`
        
        ---
        
        #### 2. Kimi (æœˆä¹‹æš—é¢)
        
        **è·å– API Keyï¼š**
        1. è®¿é—® https://platform.moonshot.cn/
        2. æ³¨å†Œ/ç™»å½•è´¦å·
        3. è¿›å…¥æ§åˆ¶å° -> API ç®¡ç†
        4. åˆ›å»º API Key
        
        **é…ç½®ä¿¡æ¯ï¼š**
        - Base URL: `https://api.moonshot.cn/v1`
        - æ¨¡å‹ï¼š`moonshot-v1-8k`, `moonshot-v1-32k`, `moonshot-v1-128k`
        
        ---
        
        #### 3. Sambanova
        
        **è·å– API Keyï¼š**
        1. è®¿é—® https://cloud.sambanova.ai/
        2. æ³¨å†Œ/ç™»å½•è´¦å·
        3. è¿›å…¥ API Keys é¡µé¢
        4. åˆ›å»º API Key
        
        **é…ç½®ä¿¡æ¯ï¼š**
        - Base URL: `https://api.sambanova.ai/v1`
        - æ¨¡å‹ï¼š`Meta-Llama-3.1-70B-Instruct`
        
        ---
        
        #### 4. Qwen (é€šä¹‰åƒé—®)
        
        **è·å– API Keyï¼š**
        1. è®¿é—® https://dashscope.console.aliyun.com/
        2. æ³¨å†Œ/ç™»å½•é˜¿é‡Œäº‘è´¦å·
        3. å¼€é€š DashScope æœåŠ¡
        4. åˆ›å»º API Key
        
        **é…ç½®ä¿¡æ¯ï¼š**
        - Base URL: `https://dashscope.aliyuncs.com/compatible-mode/v1`
        - æ¨¡å‹ï¼š`qwen-turbo`, `qwen-plus`, `qwen-max`
        """)


# ============================================================================
# AI æ™ºèƒ½å¯¹è¯
# ============================================================================

async def show_ai_chat(ctx):
    """æ˜¾ç¤º AI æ™ºèƒ½å¯¹è¯ç•Œé¢"""
    put_markdown = ctx['put_markdown']
    put_text = ctx['put_text']
    input = ctx['input']
    run_async = ctx['run_async']
    
    put_markdown("### ğŸ’¬ AI æ™ºèƒ½å¯¹è¯")
    put_markdown("ä¸ AI è¿›è¡Œå¤šè½®å¯¹è¯ï¼Œè§£ç­”é—®é¢˜ã€æä¾›å»ºè®®")
    
    # åˆå§‹åŒ–å¯¹è¯å†å²
    if 'chat_history' not in ctx:
        ctx['chat_history'] = []
    
    # æ˜¾ç¤ºå¯¹è¯å†å²
    with ctx['use_scope']('chat_history_display'):
        if not ctx['chat_history']:
            put_text("ğŸ’¡ è¾“å…¥é—®é¢˜å¼€å§‹å¯¹è¯...")
        else:
            for msg in ctx['chat_history'][-10:]:  # åªæ˜¾ç¤ºæœ€è¿‘ 10 æ¡
                if msg['role'] == 'user':
                    put_markdown(f"**ğŸ‘¤ ä½ ï¼š** {msg['content']}")
                else:
                    put_markdown(f"**ğŸ¤– AIï¼š** {msg['content']}")
    
    # è¾“å…¥æ¡†
    put_markdown("---")
    user_input = await ctx['input_group']('è¾“å…¥æ¶ˆæ¯', [
        input('æ¶ˆæ¯å†…å®¹', name='message', type='text', 
              placeholder='è¾“å…¥ä½ æƒ³é—®çš„é—®é¢˜...', required=True),
        ctx['actions']('å‘é€', [
            {'label': 'ğŸ“¤ å‘é€', 'value': 'send'},
        ], name='action')
    ])
    
    if user_input['action'] == 'send' and user_input['message']:
        await process_chat_message(ctx, user_input['message'])


async def process_chat_message(ctx, message):
    """å¤„ç†å¯¹è¯æ¶ˆæ¯"""
    put_markdown = ctx['put_markdown']
    toast = ctx['toast']
    
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
    ctx['chat_history'].append({'role': 'user', 'content': message})
    
    # æ¸…ç©ºå¹¶é‡æ–°æ˜¾ç¤º
    ctx['clear']('chat_history_display')
    
    # æ˜¾ç¤ºæ€è€ƒä¸­
    with ctx['use_scope']('thinking'):
        put_markdown("*ğŸ¤– AI æ­£åœ¨æ€è€ƒ...*")
    
    try:
        # æ„å»ºå¯¹è¯å†å²
        from deva.admin_ui.llm_service import get_gpt_response
        
        messages = []
        for msg in ctx['chat_history'][-5:]:  # ä½¿ç”¨æœ€è¿‘ 5 æ¡å†å²
            messages.append({
                'role': msg['role'],
                'content': msg['content']
            })
        
        # è·å– AI å“åº”
        prompt = message
        response = await get_gpt_response(ctx, prompt, model_type='deepseek')
        
        # æ·»åŠ  AI å“åº”åˆ°å†å²
        ctx['chat_history'].append({'role': 'assistant', 'content': response})
        
        # æ›´æ–°æ˜¾ç¤º
        ctx['clear']('thinking')
        ctx['clear']('chat_history_display')
        
        with ctx['use_scope']('chat_history_display'):
            for msg in ctx['chat_history'][-10:]:
                if msg['role'] == 'user':
                    put_markdown(f"**ğŸ‘¤ ä½ ï¼š** {msg['content']}")
                else:
                    put_markdown(f"**ğŸ¤– AIï¼š** {msg['content']}")
        
        # é‡æ–°æ˜¾ç¤ºè¾“å…¥æ¡†
        await show_ai_chat(ctx)
        
    except Exception as e:
        ctx['clear']('thinking')
        put_markdown(f"âŒ é”™è¯¯ï¼š{e}")
        toast(f'å¯¹è¯å¤±è´¥ï¼š{e}', color='error')


# ============================================================================
# AI ä»£ç ç”Ÿæˆ
# ============================================================================

async def show_ai_code_generator(ctx):
    """æ˜¾ç¤º AI ä»£ç ç”Ÿæˆå™¨"""
    put_markdown = ctx['put_markdown']
    put_button = ctx['put_button']
    run_async = ctx['run_async']
    
    put_markdown("### ğŸ’» AI ä»£ç ç”Ÿæˆ")
    put_markdown("ä½¿ç”¨ AI ç”Ÿæˆ Python ä»£ç ã€Deva ä»£ç ç­‰")
    
    # ä»£ç ç”Ÿæˆé€‰é¡¹
    put_markdown("**é€‰æ‹©ä»£ç ç±»å‹ï¼š**")
    
    put_row([
        put_button('ğŸ“ ç”Ÿæˆ Python ä»£ç ', onclick=lambda: run_async(show_python_code_gen(ctx)), color='primary'),
        put_button('ğŸ“Š ç”Ÿæˆ Deva ç­–ç•¥', onclick=lambda: run_async(show_deva_strategy_gen(ctx)), color='primary'),
        put_button('ğŸ“ˆ ç”Ÿæˆ Deva æ•°æ®æº', onclick=lambda: run_async(show_deva_datasource_gen(ctx)), color='primary'),
        put_button('âš™ï¸ ç”Ÿæˆ Deva ä»»åŠ¡', onclick=lambda: run_async(show_deva_task_gen(ctx)), color='primary'),
    ])


async def show_python_code_gen(ctx):
    """æ˜¾ç¤º Python ä»£ç ç”Ÿæˆç•Œé¢"""
    put_markdown = ctx['put_markdown']
    textarea = ctx['textarea']
    
    with ctx['popup']('ç”Ÿæˆ Python ä»£ç ', size='large', closable=True):
        put_markdown("### ğŸ“ ç”Ÿæˆ Python ä»£ç ")
        
        put_markdown("""
        **è¾“å…¥ä½ çš„éœ€æ±‚ï¼ŒAI ä¼šç”Ÿæˆå¯¹åº”çš„ Python ä»£ç ï¼š**
        
        ç¤ºä¾‹ï¼š
        - "å†™ä¸€ä¸ªå‡½æ•°ï¼Œè®¡ç®—åˆ—è¡¨ä¸­æ‰€æœ‰æ•°å­—çš„å¹³å‡å€¼"
        - "å†™ä¸€ä¸ªç±»ï¼Œå®ç°å•ä¾‹æ¨¡å¼"
        - "å†™ä¸€ä¸ªè£…é¥°å™¨ï¼Œç”¨äºè®¡ç®—å‡½æ•°æ‰§è¡Œæ—¶é—´"
        """)
        
        requirement = await ctx['input_group']('ä»£ç éœ€æ±‚', [
            textarea('éœ€æ±‚æè¿°', name='description', required=True,
                    placeholder='è¯¦ç»†æè¿°ä½ æƒ³è¦çš„ä»£ç åŠŸèƒ½...', rows=5),
            ctx['actions']('æ“ä½œ', [
                {'label': 'ğŸ¤– ç”Ÿæˆä»£ç ', 'value': 'generate'},
                {'label': 'âŒ å–æ¶ˆ', 'value': 'cancel'}
            ], name='action')
        ])
        
        if requirement['action'] == 'generate':
            await generate_code(ctx, requirement['description'], 'python')


async def show_deva_strategy_gen(ctx):
    """æ˜¾ç¤º Deva ç­–ç•¥ä»£ç ç”Ÿæˆ"""
    put_markdown = ctx['put_markdown']
    textarea = ctx['textarea']
    
    with ctx['popup']('ç”Ÿæˆ Deva ç­–ç•¥', size='large', closable=True):
        put_markdown("### ğŸ“Š ç”Ÿæˆ Deva é‡åŒ–ç­–ç•¥")
        
        put_markdown("""
        **è¾“å…¥ç­–ç•¥é€»è¾‘ï¼ŒAI ä¼šç”Ÿæˆ Deva ç­–ç•¥ä»£ç ï¼š**
        
        ç¤ºä¾‹ï¼š
        - "åŒå‡çº¿ç­–ç•¥ï¼šå½“ 5 æ—¥å‡çº¿ä¸Šç©¿ 20 æ—¥å‡çº¿æ—¶ä¹°å…¥ï¼Œä¸‹ç©¿æ—¶å–å‡º"
        - "MACD ç­–ç•¥ï¼šå½“ MACD é‡‘å‰æ—¶ä¹°å…¥ï¼Œæ­»å‰æ—¶å–å‡º"
        """)
        
        requirement = await ctx['input_group']('ç­–ç•¥éœ€æ±‚', [
            textarea('ç­–ç•¥æè¿°', name='description', required=True,
                    placeholder='è¯¦ç»†æè¿°ç­–ç•¥é€»è¾‘...', rows=5),
            ctx['input']('ç­–ç•¥åç§°', name='name', type='text', 
                        placeholder='ä¾‹å¦‚ï¼šåŒå‡çº¿ç­–ç•¥', required=True),
            ctx['actions']('æ“ä½œ', [
                {'label': 'ğŸ¤– ç”Ÿæˆä»£ç ', 'value': 'generate'},
                {'label': 'âŒ å–æ¶ˆ', 'value': 'cancel'}
            ], name='action')
        ])
        
        if requirement['action'] == 'generate':
            prompt = f"ç”Ÿæˆä¸€ä¸ª Deva é‡åŒ–ç­–ç•¥ä»£ç ï¼Œç­–ç•¥åç§°ï¼š{requirement['name']}ï¼Œç­–ç•¥é€»è¾‘ï¼š{requirement['description']}ã€‚è¦æ±‚ä½¿ç”¨ StrategyUnit ç±»ï¼Œå®ç° process æ–¹æ³•ã€‚"
            await generate_code(ctx, prompt, 'deva_strategy')


async def show_deva_datasource_gen(ctx):
    """æ˜¾ç¤º Deva æ•°æ®æºä»£ç ç”Ÿæˆ"""
    put_markdown = ctx['put_markdown']
    textarea = ctx['textarea']
    
    with ctx['popup']('ç”Ÿæˆ Deva æ•°æ®æº', size='large', closable=True):
        put_markdown("### ğŸ“ˆ ç”Ÿæˆ Deva æ•°æ®æº")
        
        put_markdown("""
        **è¾“å…¥æ•°æ®æºéœ€æ±‚ï¼ŒAI ä¼šç”Ÿæˆ Deva æ•°æ®æºä»£ç ï¼š**
        
        ç¤ºä¾‹ï¼š
        - "ä» Yahoo Finance è·å–è‚¡ç¥¨å®æ—¶æ•°æ®ï¼Œæ¯ 5 ç§’æ›´æ–°ä¸€æ¬¡"
        - "è¯»å–æœ¬åœ° CSV æ–‡ä»¶ï¼Œè§£æä¸ºå­—å…¸æ ¼å¼"
        """)
        
        requirement = await ctx['input_group']('æ•°æ®æºéœ€æ±‚', [
            textarea('æ•°æ®æºæè¿°', name='description', required=True,
                    placeholder='è¯¦ç»†æè¿°æ•°æ®æºåŠŸèƒ½...', rows=5),
            ctx['input']('æ•°æ®æºåç§°', name='name', type='text', 
                        placeholder='ä¾‹å¦‚ï¼šè‚¡ç¥¨æ•°æ®æº', required=True),
            ctx['actions']('æ“ä½œ', [
                {'label': 'ğŸ¤– ç”Ÿæˆä»£ç ', 'value': 'generate'},
                {'label': 'âŒ å–æ¶ˆ', 'value': 'cancel'}
            ], name='action')
        ])
        
        if requirement['action'] == 'generate':
            prompt = f"ç”Ÿæˆä¸€ä¸ª Deva æ•°æ®æºä»£ç ï¼Œæ•°æ®æºåç§°ï¼š{requirement['name']}ï¼ŒåŠŸèƒ½æè¿°ï¼š{requirement['description']}ã€‚è¦æ±‚ä½¿ç”¨ DataSource ç±»ï¼Œå®ç° fetch_data æ–¹æ³•ã€‚"
            await generate_code(ctx, prompt, 'deva_datasource')


async def show_deva_task_gen(ctx):
    """æ˜¾ç¤º Deva ä»»åŠ¡ä»£ç ç”Ÿæˆ"""
    put_markdown = ctx['put_markdown']
    textarea = ctx['textarea']
    
    with ctx['popup']('ç”Ÿæˆ Deva ä»»åŠ¡', size='large', closable=True):
        put_markdown("### âš™ï¸ ç”Ÿæˆ Deva ä»»åŠ¡")
        
        put_markdown("""
        **è¾“å…¥ä»»åŠ¡éœ€æ±‚ï¼ŒAI ä¼šç”Ÿæˆ Deva ä»»åŠ¡ä»£ç ï¼š**
        
        ç¤ºä¾‹ï¼š
        - "æ¯å¤©å‡Œæ™¨ 2 ç‚¹å¤‡ä»½æ•°æ®åº“"
        - "æ¯å°æ—¶æ£€æŸ¥ä¸€æ¬¡ç³»ç»ŸçŠ¶æ€ï¼Œå¼‚å¸¸æ—¶å‘é€å‘Šè­¦"
        """)
        
        requirement = await ctx['input_group']('ä»»åŠ¡éœ€æ±‚', [
            textarea('ä»»åŠ¡æè¿°', name='description', required=True,
                    placeholder='è¯¦ç»†æè¿°ä»»åŠ¡åŠŸèƒ½...', rows=5),
            ctx['input']('ä»»åŠ¡åç§°', name='name', type='text', 
                        placeholder='ä¾‹å¦‚ï¼šæ•°æ®åº“å¤‡ä»½', required=True),
            ctx['input']('æ‰§è¡Œæ—¶é—´', name='schedule', type='text', 
                        placeholder='ä¾‹å¦‚ï¼šæ¯å¤© 02:00', required=True),
            ctx['actions']('æ“ä½œ', [
                {'label': 'ğŸ¤– ç”Ÿæˆä»£ç ', 'value': 'generate'},
                {'label': 'âŒ å–æ¶ˆ', 'value': 'cancel'}
            ], name='action')
        ])
        
        if requirement['action'] == 'generate':
            prompt = f"ç”Ÿæˆä¸€ä¸ª Deva ä»»åŠ¡ä»£ç ï¼Œä»»åŠ¡åç§°ï¼š{requirement['name']}ï¼ŒåŠŸèƒ½æè¿°ï¼š{requirement['description']}ï¼Œæ‰§è¡Œæ—¶é—´ï¼š{requirement['schedule']}ã€‚è¦æ±‚ä½¿ç”¨ TaskUnit ç±»ï¼Œå®ç° execute æ–¹æ³•ã€‚"
            await generate_code(ctx, prompt, 'deva_task')


async def generate_code(ctx, prompt: str, code_type: str):
    """ç”Ÿæˆä»£ç """
    put_markdown = ctx['put_markdown']
    put_code = ctx['put_code']
    put_button = ctx['put_button']
    toast = ctx['toast']
    
    toast('æ­£åœ¨ç”Ÿæˆä»£ç ...', color='info')
    
    try:
        from deva.admin_ui.llm_service import get_gpt_response
        
        # æ„å»ºæç¤ºè¯
        system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ Python å¼€å‘è€…ï¼Œè¯·ç”Ÿæˆé«˜è´¨é‡ã€å¯è¿è¡Œçš„ä»£ç ã€‚åªè¿”å›ä»£ç ï¼Œä¸è¦å…¶ä»–è¯´æ˜ã€‚"
        full_prompt = f"{system_prompt}\n\n{prompt}"
        
        # è°ƒç”¨ AI
        code = await get_gpt_response(ctx, full_prompt, model_type='deepseek')
        
        # æ˜¾ç¤ºç”Ÿæˆçš„ä»£ç 
        with ctx['popup']('ç”Ÿæˆçš„ä»£ç ', size='large', closable=True):
            put_markdown(f"### {'ä»£ç ç±»å‹'.join({'python': 'Python', 'deva_strategy': 'Deva ç­–ç•¥', 'deva_datasource': 'Deva æ•°æ®æº', 'deva_task': 'Deva ä»»åŠ¡'})}")
            put_code(code)
            
            put_markdown("**æ“ä½œï¼š**")
            put_button('ğŸ“‹ å¤åˆ¶ä»£ç ', onclick=lambda: ctx['run_js'](f"navigator.clipboard.writeText(`{code}`)"), color='primary')
            
        toast('ä»£ç ç”ŸæˆæˆåŠŸ', color='success')
        
    except Exception as e:
        toast(f'ç”Ÿæˆå¤±è´¥ï¼š{e}', color='error')


# ============================================================================
# AI æ–‡æœ¬å¤„ç†
# ============================================================================

async def show_ai_text_processor(ctx):
    """æ˜¾ç¤º AI æ–‡æœ¬å¤„ç†å™¨"""
    put_markdown = ctx['put_markdown']
    put_button = ctx['put_button']
    run_async = ctx['run_async']
    
    put_markdown("### ğŸ“ AI æ–‡æœ¬å¤„ç†")
    put_markdown("ä½¿ç”¨ AI è¿›è¡Œæ–‡æœ¬æ‘˜è¦ã€ç¿»è¯‘ã€æ¶¦è‰²ç­‰å¤„ç†")
    
    put_row([
        put_button('ğŸ“„ æ–‡ç« æ‘˜è¦', onclick=lambda: run_async(show_text_summary(ctx)), color='info'),
        put_button('ğŸŒ ç¿»è¯‘', onclick=lambda: run_async(show_translation(ctx)), color='info'),
        put_button('âœï¸ æ¶¦è‰²', onclick=lambda: run_async(show_text_polish(ctx)), color='info'),
        put_button('ğŸ“Š åˆ†æ', onclick=lambda: run_async(show_text_analysis(ctx)), color='info'),
    ])


async def show_text_summary(ctx):
    """æ˜¾ç¤ºæ–‡æœ¬æ‘˜è¦åŠŸèƒ½"""
    put_markdown = ctx['put_markdown']
    textarea = ctx['textarea']
    
    with ctx['popup']('æ–‡ç« æ‘˜è¦', size='large', closable=True):
        put_markdown("### ğŸ“„ æ–‡ç« æ‘˜è¦ç”Ÿæˆ")
        
        put_markdown("**ç²˜è´´æ–‡ç« å†…å®¹ï¼ŒAI ä¼šè‡ªåŠ¨ç”Ÿæˆæ‘˜è¦ï¼š**")
        
        article = await ctx['input_group']('æ–‡ç« å†…å®¹', [
            textarea('æ–‡ç« å†…å®¹', name='content', required=True,
                    placeholder='ç²˜è´´æ–‡ç« å†…å®¹...', rows=10),
            ctx['actions']('æ“ä½œ', [
                {'label': 'ğŸ¤– ç”Ÿæˆæ‘˜è¦', 'value': 'generate'},
            ], name='action')
        ])
        
        if article['action'] == 'generate':
            toast = ctx['toast']
            toast('æ­£åœ¨ç”Ÿæˆæ‘˜è¦...', color='info')
            
            try:
                from deva.admin_ui.llm_service import get_gpt_response
                prompt = f"è¯·ç”¨ä¸€å¥è¯æ€»ç»“ä»¥ä¸‹æ–‡ç« ï¼š\n\n{article['content']}"
                summary = await get_gpt_response(ctx, prompt)
                
                put_markdown("### æ‘˜è¦")
                put_markdown(f"> {summary}")
                
            except Exception as e:
                toast(f'ç”Ÿæˆå¤±è´¥ï¼š{e}', color='error')


async def show_translation(ctx):
    """æ˜¾ç¤ºç¿»è¯‘åŠŸèƒ½"""
    put_markdown = ctx['put_markdown']
    textarea = ctx['textarea']
    
    with ctx['popup']('ç¿»è¯‘', size='large', closable=True):
        put_markdown("### ğŸŒ AI ç¿»è¯‘")
        
        text_data = await ctx['input_group']('ç¿»è¯‘', [
            textarea('åŸæ–‡', name='text', required=True,
                    placeholder='ç²˜è´´è¦ç¿»è¯‘çš„æ–‡æœ¬...', rows=5),
            ctx['input']('ç›®æ ‡è¯­è¨€', name='target', type='text',
                        value='ä¸­æ–‡', placeholder='ä¾‹å¦‚ï¼šä¸­æ–‡ã€è‹±æ–‡'),
            ctx['actions']('æ“ä½œ', [
                {'label': 'ğŸ¤– ç¿»è¯‘', 'value': 'translate'},
            ], name='action')
        ])
        
        if text_data['action'] == 'translate':
            toast = ctx['toast']
            toast('æ­£åœ¨ç¿»è¯‘...', color='info')
            
            try:
                from deva.admin_ui.llm_service import get_gpt_response
                prompt = f"å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘æˆ{text_data['target']}ï¼š\n\n{text_data['text']}"
                translation = await get_gpt_response(ctx, prompt)
                
                put_markdown("### ç¿»è¯‘ç»“æœ")
                put_markdown(translation)
                
            except Exception as e:
                toast(f'ç¿»è¯‘å¤±è´¥ï¼š{e}', color='error')


async def show_text_polish(ctx):
    """æ˜¾ç¤ºæ–‡æœ¬æ¶¦è‰²åŠŸèƒ½"""
    put_markdown = ctx['put_markdown']
    textarea = ctx['textarea']
    
    with ctx['popup']('æ–‡æœ¬æ¶¦è‰²', size='large', closable=True):
        put_markdown("### âœï¸ æ–‡æœ¬æ¶¦è‰²")
        
        put_markdown("**ç²˜è´´è¦æ¶¦è‰²çš„æ–‡æœ¬ï¼ŒAI ä¼šä¼˜åŒ–è¡¨è¾¾ï¼š**")
        
        text_data = await ctx['input_group']('æ–‡æœ¬æ¶¦è‰²', [
            textarea('åŸæ–‡', name='text', required=True,
                    placeholder='ç²˜è´´è¦æ¶¦è‰²çš„æ–‡æœ¬...', rows=5),
            ctx['actions']('æ“ä½œ', [
                {'label': 'ğŸ¤– æ¶¦è‰²', 'value': 'polish'},
            ], name='action')
        ])
        
        if text_data['action'] == 'polish':
            toast = ctx['toast']
            toast('æ­£åœ¨æ¶¦è‰²...', color='info')
            
            try:
                from deva.admin_ui.llm_service import get_gpt_response
                prompt = f"è¯·æ¶¦è‰²ä»¥ä¸‹æ–‡æœ¬ï¼Œä½¿å…¶æ›´æµç•…ã€ä¸“ä¸šï¼š\n\n{text_data['text']}"
                polished = await get_gpt_response(ctx, prompt)
                
                put_markdown("### æ¶¦è‰²ç»“æœ")
                put_markdown(polished)
                
            except Exception as e:
                toast(f'æ¶¦è‰²å¤±è´¥ï¼š{e}', color='error')


async def show_text_analysis(ctx):
    """æ˜¾ç¤ºæ–‡æœ¬åˆ†æåŠŸèƒ½"""
    put_markdown = ctx['put_markdown']
    textarea = ctx['textarea']
    
    with ctx['popup']('æ–‡æœ¬åˆ†æ', size='large', closable=True):
        put_markdown("### ğŸ“Š æ–‡æœ¬åˆ†æ")
        
        put_markdown("**ç²˜è´´æ–‡æœ¬ï¼ŒAI ä¼šåˆ†ææƒ…æ„Ÿã€ä¸»é¢˜ç­‰ï¼š**")
        
        text_data = await ctx['input_group']('æ–‡æœ¬åˆ†æ', [
            textarea('æ–‡æœ¬', name='text', required=True,
                    placeholder='ç²˜è´´è¦åˆ†æçš„æ–‡æœ¬...', rows=5),
            ctx['actions']('æ“ä½œ', [
                {'label': 'ğŸ¤– åˆ†æ', 'value': 'analyze'},
            ], name='action')
        ])
        
        if text_data['action'] == 'analyze':
            toast = ctx['toast']
            toast('æ­£åœ¨åˆ†æ...', color='info')
            
            try:
                from deva.admin_ui.llm_service import get_gpt_response
                prompt = f"è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬ï¼ŒåŒ…æ‹¬æƒ…æ„Ÿå€¾å‘ã€ä¸»é¢˜ã€å…³é”®è¯ç­‰ï¼š\n\n{text_data['text']}"
                analysis = await get_gpt_response(ctx, prompt)
                
                put_markdown("### åˆ†æç»“æœ")
                put_markdown(analysis)
                
            except Exception as e:
                toast(f'åˆ†æå¤±è´¥ï¼š{e}', color='error')


# ============================================================================
# AI Tab ä¸»ç•Œé¢
# ============================================================================

def render_ai_tab_ui(ctx):
    """æ¸²æŸ“ AI Tab ä¸»ç•Œé¢"""
    put_markdown = ctx['put_markdown']
    put_tabs = ctx['put_tabs']
    run_async = ctx['run_async']
    
    put_markdown("## ğŸ¤– AI åŠŸèƒ½ä¸­å¿ƒ")
    put_markdown("ä½“éªŒ Deva çš„å¼ºå¤§ AI åŠŸèƒ½ï¼ŒåŒ…æ‹¬æ¨¡å‹é…ç½®ã€æ™ºèƒ½å¯¹è¯ã€ä»£ç ç”Ÿæˆã€æ–‡æœ¬å¤„ç†ç­‰")
    
    # æ„å»º Tabs
    tabs = [
        {
            'title': 'ğŸ¤– æ¨¡å‹é…ç½®',
            'content': show_llm_config_panel(ctx)
        },
        {
            'title': 'ğŸ’¬ æ™ºèƒ½å¯¹è¯',
            'content': run_async(show_ai_chat(ctx))
        },
        {
            'title': 'ğŸ’» ä»£ç ç”Ÿæˆ',
            'content': run_async(show_ai_code_generator(ctx))
        },
        {
            'title': 'ğŸ“ æ–‡æœ¬å¤„ç†',
            'content': run_async(show_ai_text_processor(ctx))
        }
    ]
    
    put_tabs(tabs)


if __name__ == '__main__':
    # æµ‹è¯•è¿è¡Œ
    from pywebio import start_server
    from pywebio.output import *
    from pywebio.input import *
    
    def test_ai_tab():
        ctx = {
            'put_markdown': put_markdown,
            'put_tabs': put_tabs,
            'run_async': lambda f: f(),
            'input': input,
            'textarea': textarea,
            'input_group': input_group,
            'popup': popup,
            'toast': toast,
            'put_button': put_button,
            'put_table': put_table,
            'put_row': put_row,
            'put_code': put_code,
            'clear': clear,
            'use_scope': use_scope,
            'run_js': run_js,
            'radio': radio,
            'NB': None,
            'log': None,
            'warn': None,
        }
        render_ai_tab_ui(ctx)
    
    start_server(test_ai_tab, port=8080, debug=True)
